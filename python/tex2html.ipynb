{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "from tex2svg import tex2svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scanner():\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.line = 1\n",
    "        self.position = 0\n",
    "        \n",
    "    def get(self):\n",
    "        c = self.file.read(1)\n",
    "        if c:\n",
    "            if c == '\\n':\n",
    "                self.line += 1\n",
    "                self.position = 0\n",
    "            else:\n",
    "                self.position += 1\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "\n",
    "    T_TEXT = 0\n",
    "    T_COMMAND = 1\n",
    "    T_SEPARATOR = 2\n",
    "    T_WHITESPACE = 3\n",
    "    T_NEWLINE = 4\n",
    "    T_EOF = 5\n",
    "    \n",
    "    def __init__(self, T, line, position, data = ''):\n",
    "        self.type = T\n",
    "        self.line = line\n",
    "        self.position = position\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer():\n",
    "    \n",
    "    def __init__(self, scanner):\n",
    "        self.scanner = scanner\n",
    "        self.current_token = None\n",
    "        self.tmp = ''\n",
    "        self.tmp_line = 0\n",
    "        self.tmp_position = 0\n",
    "        \n",
    "    def tokenize(self, expr):\n",
    "        if expr in ['{', '}', '\\\\[', '\\\\]', '$', '\\\\&', '\\\\#', '\\\\\\\\', '\\\\{', '\\\\}', '\\;']:\n",
    "            return Token(Token.T_SEPARATOR, self.tmp_line, self.tmp_position, expr)\n",
    "        \n",
    "        if re.match(r'\\A\\\\\\w+\\Z', expr):\n",
    "            return Token(Token.T_COMMAND, self.tmp_line, self.tmp_position, expr)\n",
    "\n",
    "        if re.match(r'\\n\\Z', expr):\n",
    "            return Token(Token.T_NEWLINE, self.tmp_line, self.tmp_position, expr)\n",
    "        \n",
    "        if re.match(r'\\A[^\\S\\n]+\\Z', expr):\n",
    "            return Token(Token.T_WHITESPACE, self.tmp_line, self.tmp_position, expr)\n",
    "        \n",
    "        if re.match(r'\\A[^{}\\\\$\\s][^{}\\\\$\\n]*\\Z', expr):\n",
    "            return Token(Token.T_TEXT, self.tmp_line, self.tmp_position, expr)\n",
    "\n",
    "        return None\n",
    "        \n",
    "    def get_token(self):\n",
    "        # Read characters until a new Token is produced\n",
    "        while True:\n",
    "            c = self.scanner.get()\n",
    "\n",
    "            # End of file\n",
    "            if not c:\n",
    "                if self.tmp == '':\n",
    "                    return Token(Token.T_EOF, -1, -1, '')\n",
    "                    \n",
    "                token = self.tokenize(self.tmp)\n",
    "                if not token:\n",
    "                    raise Exception('Unexpected token \\'{}\\''.format(self.tmp))\n",
    "                self.current_token = None\n",
    "                self.tmp = ''\n",
    "                return token\n",
    "            \n",
    "            \n",
    "            # Comments: marks the end of a token (if there currently is one),\n",
    "            # then continue discarding characters until a newline appears\n",
    "            if c in ['%']:\n",
    "                token = None\n",
    "                \n",
    "                if self.tmp != '':\n",
    "                    if not self.current_token:\n",
    "                        raise Exception('Unknown token \\'{}\\''.format(self.tmp))\n",
    "                    token = self.current_token\n",
    "                \n",
    "                while self.scanner.get() != '\\n':\n",
    "                    pass\n",
    "                \n",
    "                self.tmp = '' # LaTeX will then omit the newline\n",
    "                self.tmp_line = self.scanner.line\n",
    "                self.tmp_position = self.scanner.position\n",
    "                self.current_token = self.tokenize(self.tmp)\n",
    "                \n",
    "                return token if token else self.get_token()\n",
    "            \n",
    "            # Try to enlarge the token if possible\n",
    "            token = self.tokenize(self.tmp + c)\n",
    "            if token:\n",
    "                self.current_token = token\n",
    "                if self.tmp == '':\n",
    "                    self.tmp_line = self.scanner.line\n",
    "                    self.tmp_position = self.scanner.position\n",
    "                self.tmp += c\n",
    "                continue\n",
    "\n",
    "            # If we also did not succeed before, hope that it will make sense later\n",
    "            if not self.current_token:\n",
    "                self.tmp += c\n",
    "                continue\n",
    "\n",
    "            # Return the last valid token\n",
    "            token = self.current_token\n",
    "            self.tmp = c\n",
    "            self.tmp_line = self.scanner.line\n",
    "            self.tmp_position = self.scanner.position\n",
    "            self.current_token = self.tokenize(self.tmp)\n",
    "            return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        self.current_token = None\n",
    "        self.prefix = ''\n",
    "        self.topics = {}\n",
    "    \n",
    "    def set_prefix(self, prefix):\n",
    "        self.prefix = prefix\n",
    "    \n",
    "    def get_topics(self):\n",
    "        return self.topics\n",
    "    \n",
    "    def next_token(self):\n",
    "        self.current_token = self.lexer.get_token()\n",
    "            \n",
    "    def found(self, token_type = None, data = None):\n",
    "        if self.current_token == None:\n",
    "            self.next_token()\n",
    "        return token_type == None or (self.current_token.type == token_type and (data == None or data == self.current_token.data))\n",
    "        \n",
    "    def consume(self, token_type = None, data = None):            \n",
    "        if self.found(token_type, data):\n",
    "            token = self.current_token\n",
    "            self.current_token = None\n",
    "            return token\n",
    "        else:\n",
    "            raise Exception('Expected \\'{}\\' but found \\'{}\\' [{}]'.format(data, self.current_token.data, self.current_token.type))\n",
    "    \n",
    "    def omit_whitespace(self, omit_newlines = True):\n",
    "        while self.found(Token.T_WHITESPACE) or (omit_newlines and self.found(Token.T_NEWLINE)):\n",
    "            self.consume()\n",
    "    \n",
    "    def parse(self, tex_file):        \n",
    "        # Create Scanner & Lexer\n",
    "        self.scanner = Scanner(open(tex_file, 'r'))\n",
    "        self.lexer = Lexer(self.scanner)\n",
    "        self.current_token = None\n",
    "        \n",
    "        # Keep parsing topics until end of file (ignoring newlines)\n",
    "        self.omit_whitespace()\n",
    "        while not self.found(Token.T_EOF):\n",
    "            self.parse_topic()\n",
    "            self.omit_whitespace()\n",
    "            \n",
    "    def parse_topic(self):\n",
    "        # \\begin{topic}{identifier}{name} ... \\end{topic}\n",
    "        self.consume(Token.T_COMMAND, '\\\\begin')\n",
    "        self.consume(Token.T_SEPARATOR, '{')\n",
    "        self.consume(Token.T_TEXT, 'topic')\n",
    "        self.consume(Token.T_SEPARATOR, '}')\n",
    "        self.consume(Token.T_SEPARATOR, '{')\n",
    "        suffix = self.consume(Token.T_TEXT).data\n",
    "        self.consume(Token.T_SEPARATOR, '}')\n",
    "        self.consume(Token.T_SEPARATOR, '{')\n",
    "        name = self.consume(Token.T_TEXT).data\n",
    "        self.consume(Token.T_SEPARATOR, '}')\n",
    "        \n",
    "        identifier = self.prefix + ':' + suffix\n",
    "        if identifier in self.topics:\n",
    "            raise Exception('Identifier ' + identifier + ' already used')\n",
    "        \n",
    "        print('{} [{}]'.format(name, identifier))\n",
    "        self.topics[identifier] = name\n",
    "        \n",
    "        self.output = open(self.output_dir + '/' + self.prefix + '-' + suffix + '.html', 'w')\n",
    "        self.parse_environment('topic')\n",
    "        self.output.close()\n",
    "    \n",
    "    def parse_begin_environment(self):\n",
    "        self.consume(Token.T_COMMAND, '\\\\begin')\n",
    "        self.consume(Token.T_SEPARATOR, '{')\n",
    "        env = self.consume(Token.T_TEXT).data\n",
    "        self.consume(Token.T_SEPARATOR, '}')\n",
    "        args = []\n",
    "        while self.found(Token.T_SEPARATOR, '{'):\n",
    "            self.consume()\n",
    "            args.append(self.consume(Token.T_TEXT).data)\n",
    "            self.consume(Token.T_SEPARATOR, '}')\n",
    "        \n",
    "        self.parse_environment(env, args)\n",
    "    \n",
    "    def parse_environment(self, env, args = []):\n",
    "        self.omit_whitespace()\n",
    "        \n",
    "        if env == 'enumerate':\n",
    "            self.parse_environment_list(True, args[0])\n",
    "        elif env == 'itemize':\n",
    "            self.parse_environment_list(False)\n",
    "        else:\n",
    "            self.parse_content()\n",
    "            \n",
    "        self.omit_whitespace()\n",
    "        \n",
    "        self.consume(Token.T_COMMAND, '\\\\end')\n",
    "        self.consume(Token.T_SEPARATOR, '{')\n",
    "        self.consume(Token.T_TEXT, env)\n",
    "        self.consume(Token.T_SEPARATOR, '}')\n",
    "            \n",
    "    def parse_environment_list(self, ordered, item_type = '1'):\n",
    "        self.output.write('<ol type=\"{}\">'.format(item_type) if ordered else '<ul>')\n",
    "        \n",
    "        first_item = True\n",
    "        while self.found(Token.T_COMMAND, '\\\\item'):\n",
    "            self.consume()\n",
    "            self.output.write('<li>' if first_item else '</li><li>')\n",
    "            first_item = False\n",
    "            self.parse_content()\n",
    "                \n",
    "        if not first_item:\n",
    "            self.output.write('</li>')\n",
    "        self.output.write('</ol>' if ordered else '</ul>')    \n",
    "            \n",
    "    def parse_content(self):\n",
    "        while True:\n",
    "            if self.found(Token.T_TEXT) or self.found(Token.T_WHITESPACE):\n",
    "                token = self.consume()\n",
    "                self.output.write(self.special_chars(token.data))\n",
    "                continue\n",
    "            \n",
    "            if self.found(Token.T_NEWLINE):\n",
    "                self.consume()\n",
    "                self.omit_whitespace(omit_newlines = False)\n",
    "                if self.found(Token.T_NEWLINE):\n",
    "                    self.omit_whitespace()\n",
    "                    self.output.write('<br/>')\n",
    "                self.output.write('\\n')\n",
    "                continue\n",
    "            \n",
    "            if self.found(Token.T_SEPARATOR, '$'):\n",
    "                self.parse_inline_math()\n",
    "                continue\n",
    "\n",
    "            if self.found(Token.T_SEPARATOR, '\\\\['):\n",
    "                self.parse_display_math()\n",
    "                continue\n",
    "\n",
    "            if self.found(Token.T_COMMAND, '\\\\textbf'):\n",
    "                self.parse_textbf()\n",
    "                continue\n",
    "\n",
    "            if self.found(Token.T_COMMAND, '\\\\textit'):\n",
    "                self.parse_textit()\n",
    "                continue\n",
    "                \n",
    "            if self.found(Token.T_COMMAND, '\\\\tref'):\n",
    "                self.parse_tref()\n",
    "                continue\n",
    "                \n",
    "            if self.found(Token.T_COMMAND, '\\\\begin'):\n",
    "                self.parse_begin_environment()\n",
    "                continue\n",
    "                \n",
    "            break\n",
    "        \n",
    "    def parse_inline_math(self):\n",
    "        self.consume(Token.T_SEPARATOR, '$')\n",
    "        s = ''\n",
    "        while not self.found(Token.T_SEPARATOR, '$'):\n",
    "            s += self.consume().data\n",
    "        self.consume()\n",
    "        self.output.write('<span class=\"math inline\">\\\\(' + s + '\\\\)</span>')\n",
    "    \n",
    "    def parse_display_math(self):\n",
    "        self.consume(Token.T_SEPARATOR, '\\\\[')\n",
    "        s = ''\n",
    "        while not self.found(Token.T_SEPARATOR, '\\\\]'):\n",
    "            s += self.consume().data\n",
    "        self.consume()\n",
    "        \n",
    "        if 'tikzcd' not in s:\n",
    "            self.output.write('<span class=\"math display\">\\\\[' + s + '\\\\]</span>')\n",
    "        else:\n",
    "            svg = self.math_to_svg(s)\n",
    "            if svg == False:\n",
    "                raise Exception('failed to compile display math')\n",
    "            self.output.write('<img class=\"display-math-svg\" src=\"data/{}\" alt />'.format(svg))\n",
    "        \n",
    "    def parse_textbf(self):\n",
    "        self.consume(Token.T_COMMAND, '\\\\textbf')\n",
    "        self.consume(Token.T_SEPARATOR, '{')\n",
    "        self.output.write('<b>')\n",
    "        self.parse_content()\n",
    "        self.consume(Token.T_SEPARATOR, '}')\n",
    "        self.output.write('</b>')\n",
    "        \n",
    "    def parse_textit(self):\n",
    "        self.consume(Token.T_COMMAND, '\\\\textit')\n",
    "        self.consume(Token.T_SEPARATOR, '{')\n",
    "        self.output.write('<i>')\n",
    "        self.parse_content()\n",
    "        self.consume(Token.T_SEPARATOR, '}')\n",
    "        self.output.write('</i>')\n",
    "        \n",
    "    def parse_tref(self):\n",
    "        self.consume(Token.T_COMMAND, '\\\\tref')\n",
    "        self.consume(Token.T_SEPARATOR, '{')\n",
    "        identifier = self.consume(Token.T_TEXT).data\n",
    "        self.consume(Token.T_SEPARATOR, '}')\n",
    "        self.consume(Token.T_SEPARATOR, '{')\n",
    "        text = self.consume(Token.T_TEXT).data\n",
    "        self.consume(Token.T_SEPARATOR, '}')\n",
    "\n",
    "        if ':' not in identifier:\n",
    "            identifier = self.prefix + ':' + identifier\n",
    "        \n",
    "        self.output.write('<a href=\"javascript:gotoTopic(\\'{}\\');\">{}</a>'.format(identifier, text))\n",
    "    \n",
    "    def special_chars(self, s):\n",
    "        if '`' in s:\n",
    "            s = s.replace('`', '&lsquo;')\n",
    "        if '\\'' in s:\n",
    "            s = s.replace('\\'', '&rsquo;')\n",
    "        return s\n",
    "    \n",
    "    def math_to_svg(self, tex):\n",
    "        # See if the svg has been created before\n",
    "        tex_hash = hashlib.sha1(tex.encode()).hexdigest()\n",
    "        svg_file_relative = 'svg/' + tex_hash + '.svg'\n",
    "        svg_file = self.output_dir + '/' + svg_file_relative\n",
    "        if os.path.isfile(svg_file):\n",
    "            return svg_file_relative\n",
    "\n",
    "        # Create tmp directory if not exists\n",
    "        tmp_dir = self.output_dir + '/svg/_tmp_'\n",
    "        if not os.path.isdir(tmp_dir):\n",
    "            os.mkdir(tmp_dir)\n",
    "\n",
    "        # Create tex file\n",
    "        f = open(tmp_dir + '/math.tex', 'w')\n",
    "        f.writelines([\n",
    "            '\\\\documentclass{standalone}\\n',\n",
    "            '\\\\usepackage{tikz-cd}\\n',\n",
    "            '\\\\newcommand{\\\\Hom}{\\\\textup{Hom}}\\n',\n",
    "            '\\\\newcommand{\\\\bdot}{\\\\bullet}\\n',\n",
    "            '\\\\begin{document}\\n',\n",
    "            '$\\\\displaystyle ' + tex + '$\\n',\n",
    "            '\\\\end{document}\\n'\n",
    "        ])\n",
    "        f.close()\n",
    "\n",
    "        # Create svg\n",
    "        try:\n",
    "            tex2svg(tmp_dir + '/math.tex', svg_file)\n",
    "            return svg_file_relative\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sheaf [AG:sheaf]\n",
      "constant sheaf [AG:constant-sheaf]\n",
      "stalk [AG:stalk]\n",
      "associated sheaf [AG:associated-sheaf]\n",
      "direct image sheaf [AG:direct-image-sheaf]\n",
      "inverse image sheaf [AG:inverse-image-sheaf]\n",
      "flasque sheaf [AG:flasque-sheaf]\n",
      "skyscraper sheaf [AG:skyscraper-sheaf]\n",
      "sheaf hom [AG:sheaf-hom]\n"
     ]
    }
   ],
   "source": [
    "# parser = Parser('/Users/jessevogel/Projects/math-definitions/data/')\n",
    "# parser.set_prefix('AG')\n",
    "# parser.parse('/Users/jessevogel/Projects/math-definitions/tex/sheaves.tex')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
