\begin{topic}{vector-space}{vector space}
    A \textbf{vector space} over a \tref{CA:field}{field} $k$ is a $k$-module. Its elements are called \textit{vectors}.
    % Concretely, ...
\end{topic}

\begin{topic}{vector-subspace}{vector-subspace}
    Let $V$ be a \tref{vector-space}{vector space}. A \textbf{subspace} of $V$ is a subset $W \subset V$ which is a vector space itself.
\end{topic}

\begin{topic}{linearly-independent}{linearly independent}
    Let $V$ be a \tref{vector-space}{vector space}. A set of vectors $S \subset V$ is said to be \textbf{linearly independent} if for any relation
    \[ \sum_{i = 1}^{n} a_i v_i = 0, \qquad \text{with $v_i \in S$ and $a_i \in k$,} \]
    one has $a_i = 0$.
\end{topic}

\begin{topic}{span}{span}
    Let $V$ be a \tref{vector-space}{vector space}, and $S \subset V$ a subset. The \textbf{span} of $S$ is the subspace of all linear combinations of vectors in $S$.
    \[ \text{span}(S) = \left\{ \sum_{i = 1}^{n} a_i v_i \text{ for any } v_i \in S \text{ and } a_i \in k \right\} \]
\end{topic}

\begin{topic}{basis}{basis}
    Let $V$ be a \tref{vector-space}{vector space}. A set of vectors in $V$ is a \textbf{basis} for $V$ if it \tref{span}{spans} $V$ and is \tref{linearly-independent}{linearly independent}.
    
    Equivalently, a set is a basis if each vector in $V$ can be expressed uniquely as a linear combination of vectors in the set.
\end{topic}

\begin{topic}{dimension}{dimension}
    Let $V$ be a \tref{vector-space}{vector space}. The \textbf{dimension} of $V$ is the cardinality of any basis of $V$. It is often denoted by $\dim(V)$.
\end{topic}

\begin{topic}{linear-map}{linear map}
    A \textbf{linear map} is a map $f : V \to W$ between \tref{vector-space}{vector spaces} such that
    \begin{itemize}
        \item $f(v + v') = f(v) + f(v')$,
        \item $f(av) = a f(v)$,
    \end{itemize}
    for all vectors $v, v' \in V$ and scalars $a \in k$.
\end{topic}

\begin{topic}{matrix-representation}{matrix representation}
    Let $V$ and $W$ be finite-dimensional \tref{vector-space}{vector spaces}, and $T : V \to W$ a \tref{linear-map}{linear transformation}. If $(v_1, v_2, \ldots, v_n)$ and $(w_1, w_2, \ldots, w_m)$ are (ordered) \tref{basis}{bases} of $V$ and $W$, respectively, then the \textbf{matrix representation} of $T$ is the $m \times n$ matrix
    \[ A = \begin{pmatrix} a_{11} & a_{21} & \cdots & a_{n1} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{pmatrix} \]
    such that $i$-th component of $T(v_j)$ (w.r.t. the basis for $W$) is $a_{ij}$.
\end{topic}

\begin{topic}{matrix-multiplication}{matrix multiplication}
    Let $A = (a_{ij})$ be an $m \times n$ matrix, and $B = (b_{ij})$ be an $n \times \ell$ matrix. The \textbf{matrix product} $AB$ is the $m \times \ell$ matrix $C = (c_{ij})$ given by
    \[  c_{ij} = \sum_{k = 1}^{n} a_{ik} b_{kj} . \]
\end{topic}

\begin{topic}{matrix-transpose}{matrix transpose}
    The \textbf{transpose} of a matrix $A = (a_{ij})$ is the matrix $A^T = (b_{ij})$ given by $b_{ij} = a_{ji}$.
\end{topic}

% \begin{topic}{row-echelon-form}{(reduced) row-echelon form}
    
% \end{topic}

\begin{topic}{invertible-matrix}{invertible matrix}
    An $n \times n$ matrix $A$ is \textbf{invertible} if there exists an $n \times n$ matrix $B$ such that $AB = BA = I$. In this case, $B$ is called the \textbf{inverse} of $A$, and is the unique matrix with this property.
    
    If $A$ is not invertible, it is called \textbf{singular}.
\end{topic}

\begin{example}{invertible-matrix}
    When $n = 2$, we have
    \[ \begin{pmatrix} a & b \\ c & d \end{pmatrix} = \frac{1}{ad - bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix} . \]
\end{example}

\begin{topic}{matrix-rank}{matrix rank}
    The \textbf{rank} of an $m \times n$ matrix $A$ the \tref{dimension}{dimension} of its \textit{row space} $\im A$.
\end{topic}

\begin{topic}{nullspace}{nullspace}
    The \textbf{nullspace}, or \textbf{kernel}, of a \tref{linear-map}{linear transformation} $T : V \to W$ is the subspace
    \[ \left\{ v \in V \text{ such that } T(v) = 0 \right\} \subset V . \]
\end{topic}

\begin{topic}{projection}{projection}
    A \textbf{projection} is a \tref{linear-map}{linear transformation} $P : V \to V$ with $P^2 = P$.
\end{topic}

\begin{topic}{determinant}{determinant}
    The \textbf{determinant} of an $n \times n$ matrix $A$ is the scalar given by
    \[ \det(A) = \sum_{\sigma \in S_n} \left( \text{sign}(\sigma) \prod_{i = 1}^{n} a_{i, \sigma(i)} \right) . \]
    It has the property that $A$ is \tref{invertible-matrix}{invertible} if and only if $\det(A) \ne 0$.
    
    Also, the determinant of $A$ is the scalar corresponding to the map induced on the top exterior power of $V$,
    \[ \wedge^n A : \wedge^n V \to \wedge^n V . \]
\end{topic}

\begin{example}{determinant}
    When $n = 2$,
    \[ \det \begin{pmatrix} a & b \\ c & d \end{pmatrix} = ad - bc . \]
    When $n = 3$,
    \[ \det \begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix} = a(ei - fh) - b(di - fg) + c(dh - eg) . \]
\end{example}

\begin{topic}{cofactor-matrix}{cofactor matrix}
    Let $A = (a_{ij})$ be an $n \times n$ matrix. The \textbf{cofactor} of an entry $a_{ij}$ of $A$ is
    \[ a'_{ij} = (-1)^{i + j} \det(A_{ij}), \]
    where $A_{ij}$ is the matrix obtained from $A$ by removing the $i$-th row and $j$-th column. The \textbf{cofactor matrix} of $A$ is then the $n \times n$ matrix $A' = (a'_{ij})$.
\end{topic}

\begin{topic}{adjoint-matrix}{adjoint matrix}
    Let $A$ be an $n \times n$ matrix. The \textbf{adjoint} of $A$ is the \tref{matrix-transpose}{transpose} of its \tref{cofactor-matrix}{cofactor matrix}:
    \[ \text{adj}(A) = (A')^T . \]
    It has the property that
    \[ \text{adj}(A) A = \det(A) I , \]
    so in particular, when $\det(A) \ne 0$, one has
    \[ A^{-1} = \frac{1}{\det(A)} \text{adj}(A) . \]
\end{topic}

\begin{topic}{eigenvalue}{eigenvalue/eigenvector}
    Let $A$ be an $n \times n$ matrix. A scalar $\lambda$ is an \textbf{eigenvalue} of $A$ if there exists a nonzero vector $v$ such that $A v = \lambda v$. The vector $v$ is then an \textbf{eigenvector} of $A$ corresponding to $\lambda$.
\end{topic}

\begin{topic}{characteristic-polynomial}{characteristic polynomial}
    Let $A$ be an $n \times n$ matrix. The \textbf{characteristic polynomial} of $A$ is the polynomial of degree $n$ given by
    \[ p_A(\lambda) = \det(A - \lambda I) . \]
    Its roots are precisely the \tref{eigenvalue}{eigenvalues} of $A$.
\end{topic}

\begin{topic}{diagonalization}{diagonalization}
    A \textbf{diagonalization} of an $n \times n$ matrix $A$ consists of an invertible matrix $C$ and a diagonal matrix $D$ such that
    \[ D = C A C^{-1} . \]
    If such a diagonalization exists, $A$ is said to be \textbf{diagonalizable}.
\end{topic}

\begin{example}{diagonalization}
    Let $A$ be a diagonalizable matrix. Note that we can write
    \[ AC = DC , \]
    with $C$ invertible and $D$ diagonal. From this expression it is clear that the columns of $C$ must be \tref{eigenvalue}{eigenvectors} of $A$, and that $D$ contains the corresponding eigenvalues. The matrix $C$ being invertible means that $A$ has $n$ linearly independent eigenvectors.
\end{example}

\begin{topic}{algebraic-geometric-multiplicity}{algebraic/geometric multiplicity}
    Let $A$ be an $n \times n$ matrix. The \textbf{algebraic multiplicity} of an eigenvalue $\lambda$ of $A$ is its multiplicity as a root of the \tref{characteristic-polynomial}{characteristic polynomial} of $A$.
    
    The \textbf{geometric multiplicity} is the \tref{dimension}{dimension} of the corresponding \textit{eigenspace}
    \[ E_\lambda = \{ v \in V : A v = \lambda v \} . \]
\end{topic}

\begin{example}{algebraic-geometric-multiplicity}
    Consider the matrix
    \[ A = \begin{pmatrix} 2 & 1 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{pmatrix} . \]
    Its characteristic polynomial is $p_A(\lambda) = (2 - \lambda)^2 (3 - \lambda)$, so its eigenvalues are $\lambda = 2$ (with alg. multiplicity $2$) and $\lambda = 3$ (with alg. multiplicity $1$). However, both have a geometric multiplicity of $1$ as
    \[ E_2 = \ker(A - 2I) = \text{span}(1, 0, 0) \quad \text{ and } \quad E_3 = \ker(A - 3I) = \text{span}(0, 0, 1) . \]
\end{example}

\begin{topic}{dual-vector-space}{dual vector space}
    Given a \tref{vector-space}{vector space} $V$, the \textbf{dual vector space} $V^*$ is defined as the vector space of all \tref{linear-map}{linear maps} $f : V \to k$.
\end{topic}

\begin{topic}{grassmannian}{Grassmannian}
    Given a \tref{LA:vector-space}{vector space} $V$, and $r \ge 0$, the \textbf{Grassmannian} $\text{Gr}(r, V)$ is a space that parametrizes all $r$-dimensional \tref{LA:vector-subspace}{linear subspaces} of $V$.
\end{topic}

\begin{example}{grassmannian}
    When $r = 1$, the Grassmanian $\text{Gr}(1, V)$ parametrizes all lines in $V$ through the origin, that is, $\text{Gr}(1, V) = \PP(V)$.
\end{example}

\begin{topic}{general-linear-group}{general linear group}
    The \textbf{general linear group} of degree $n$ over a field $k$, denoted $\text{GL}_n(k)$, is the \tref{GT:group}{group} of $n \times n$ \tref{invertible-matrix}{invertible matrices} with the operation of matrix multiplication.
\end{topic}

\begin{topic}{special-linear-group}{special linear group}
    The \textbf{special linear group} of degree $n$ over a field $k$, denoted $\text{SL}_n(k)$, is the \tref{GT:group}{group} of $n \times n$ \tref{invertible-matrix}{invertible matrices} with \tref{determinant}{determinant} $1$, with the operation of matrix multiplication.
\end{topic}

\begin{topic}{orthogonal-matrix}{orthogonal matrix}
    An $n \times n$ matrix $A$ is \textbf{orthogonal} if $A A^T = I$.
\end{topic}

\begin{topic}{orthogonal-group}{(special) orthogonal group}
    The \textbf{orthogonal group} of degree $n$ over a field $k$, denoted $\text{O}(n, k)$, is the \tref{GT:group}{group} of $n \times n$ \tref{orthogonal-matrix}{orthogonal matrices} with the operation of matrix multiplication.
    
    The \textbf{special orthogonal group} is the \tref{GT:subgroup}{subgroup} $\text{SO}(n, k) \subset \text{O}(n, k)$ of matrices with \tref{determinant}{determinant} $1$.
\end{topic}

\begin{topic}{unitary-matrix}{unitary matrix}
    An $n \times n$ matrix $U$ over $\CC$ is \textbf{unitary} if $UU^H = I$, where $U^H$ is its conjugate \tref{matrix-transpose}{transpose}.
\end{topic}

\begin{topic}{unitary-group}{(special) unitary group}
    The \textbf{unitary group} of degree $n$, denoted $\text{U}(n)$, is the \tref{GT:group}{group} of $n \times n$ \tref{unitary-matrix}{unitary matrices} with the operation of matrix multiplication.
    
    The \textbf{special unitary group} is the \tref{GT:subgroup}{subgroup} $\text{SU}(n) \subset \text{U}(n)$ of matrices with \tref{determinant}{determinant} $1$.
\end{topic}

\begin{topic}{symplectic-matrix}{symplectic matrix}
    A $2n \times 2n$ matrix $M$ is \textbf{symplectic} if $M^T \Omega M = \Omega$, where $\Omega = \begin{pmatrix} 0 & I_n \\ -I_n & 0 \end{pmatrix}$.
\end{topic}

\begin{topic}{symplectic-group}{symplectic group}
    The \textbf{symplectic group} of degree $2n$ over a field $k$, denoted $\text{Sp}(2n, k)$, is the \tref{GT:group}{group} of $2n \times 2n$ \tref{symplectic-matrix}{symplectic matrices} with the operation of matrix multiplication.
\end{topic}

\begin{topic}{hermitian-matrix}{Hermitian matrix}
    An $n \times n$ complex matrix $A$ is \textbf{Hermitian} if it is equal to its conjugate \tref{matrix-transpose}{transpose}, that is $A = A^H$.
\end{topic}

\begin{example}{hermitian-matrix}
    A Hermitian matrix $A$ always has real eigenvalues. Namely, if $Av = \lambda v$, then
    \[ \lambda \norm{v} = v^H A v = v^H A^H v = (A v)^H v = \overline{\lambda} v^H v = \overline{\lambda} \norm{v} , \]
    which shows $\lambda$ is real.
\end{example}

\begin{topic}{pin-group}{pin group}
    The \textbf{pin group} $\text{Pin}(V, q)$ of a \tref{vector-space}{vector space} $V$ over $k$ with a \tref{quadratic-form}{quadratic form} $q : V \to k$ is the subgroup of the units of the \tref{clifford-algebra}{Clifford algebra} $\text{Cl}(V, q)$ of elements of the form $v_1 v_2 \cdots v_k$ with $q(v_i) = 1$.
    
    As a special case, $\text{Pin}(n) = \text{Pin}(\RR^n, \langle \cdot, \cdot \rangle)$.
\end{topic}

\begin{topic}{spin-group}{spin group}
    The \textbf{spin group} $\text{Spin}(V, q)$ of a \tref{vector-space}{vector space} $V$ over $k$ with a \tref{quadratic-form}{quadratic form} $q : V \to k$ is the subgroup of the units of the \tref{clifford-algebra}{Clifford algebra} $\text{Cl}(V, q)$ of elements of the form $v_1 v_2 \cdots v_{2k}$ with $q(v_i) = 1$.

    As a special case, $\text{Spin}(n) = \text{Spin}(\RR^n, \langle \cdot, \cdot \rangle)$.
    
    The \textbf{spin group} $\text{Spin}(n)$ is the universal (double) cover of the \tref{orthogonal-group}{special orthogonal group} $\text{SO}(n)$.
\end{topic}

\begin{example}{spin-group}
    Let $\{ e_i \}$ be an orthonormal basis for $V$ w.r.t. $q$. Define an \textit{antiautomorphism} $t : \text{Cl}(V, q) \to \text{Cl}(V, q)$ by $(e_i e_j \cdots e_k)^t = e_k \cdots e_j e_i$ and extending linearly. Then define the automorphism $\alpha : \text{Cl}(V, q) \to \text{Cl}(V, q)$ by $\alpha(v) = -v$ for $v \in V$. Let $a^*$ denote $\alpha(a)^t$ for $a \in \text{Cl}(V, q)$.
    
    With this notation, an explicit double cover of $\text{O}(V, q)$ by $\text{Pin}(V, q)$ is given by
    \[ \rho : \text{Pin}(V, q) \to \text{O}(V, q), \quad \rho(a) v = a v a^* . \]
    Restricting $\rho$ to $\text{Spin}(V, q)$ gives a double cover of $\text{SO}(V, q)$.
\end{example}

\begin{topic}{clifford-algebra}{Clifford algebra}
    The \textbf{Clifford algebra} of a \tref{vector-space}{vector space} $V$ over $k$ with a \tref{quadratic-form}{quadratic form} $q : V \to k$ is the algebra over $k$
    \[ \text{Cl}(V, q) = T(V) / (v \otimes v - q(v)) , \]
    where $T(V)$ denotes the \tref{CA:tensor-algebra}{tensor algebra} of $V$.
\end{topic}

\begin{topic}{quadratic-form}{quadratic form}
    A \textbf{quadratic form} on a \tref{vector-space}{vector space} over $k$ is a function $q : V \to k$ such that
    \begin{itemize}
        \item $q(av) = a^2 v$ for all $a \in k$ and $v \in V$,
        \item the \textit{polarization map} $V \times V \to k : (v, w) \mapsto q(v + w) - q(v) - q(w)$ is a bilinear form.
    \end{itemize}
\end{topic}

\begin{example}{quadratic-form}
    On $V = \RR^n$, the standard quadratic form is the norm-function,
    \[ \norm{\cdot} : \RR^n \to \RR, \quad v \mapsto \norm{v} . \]
\end{example}

\begin{topic}{generalized-eigenvalue}{generalized eigenvalue/eigenvector}
    Let $A$ be an $n \times n$ matrix. A scalar $\lambda$ is a \textbf{generalized eigenvalue} if there exists a vector $v$ and integer $N \ge 1$ such that $(A - \lambda I)^N v = 0$. The vector $v$ is called the \textbf{generalized eigenvector} corresponding to $\lambda$.
\end{topic}
